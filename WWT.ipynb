{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8119bb09",
      "metadata": {
        "id": "8119bb09"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, Flatten, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5809ac6",
      "metadata": {},
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55ef445a",
      "metadata": {
        "id": "55ef445a"
      },
      "outputs": [],
      "source": [
        "order_data = pd.read_csv(\"order_data.csv\")\n",
        "customer_data = pd.read_csv(\"customer_data.csv\")\n",
        "test_data = pd.read_csv('test_data_question.csv')\n",
        "# store_data = pd.read_csv(\"store_data.csv\") # Not directly used in the current preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fb8c503",
      "metadata": {},
      "outputs": [],
      "source": [
        "order_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "546db862",
      "metadata": {},
      "outputs": [],
      "source": [
        "customer_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "153de966",
      "metadata": {},
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd1a8c54",
      "metadata": {},
      "source": [
        "1. Initial Data Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "930902d7",
      "metadata": {
        "id": "930902d7"
      },
      "outputs": [],
      "source": [
        "# Handle null values for CUSTOMER_TYPE\n",
        "customer_data['CUSTOMER_TYPE'] = customer_data['CUSTOMER_TYPE'].fillna('Registered')\n",
        "\n",
        "# Convert ORDER_CREATED_DATE to datetime and extract features\n",
        "order_data['ORDER_CREATED_DATE'] = pd.to_datetime(order_data['ORDER_CREATED_DATE'])\n",
        "order_data['YEAR'] = order_data['ORDER_CREATED_DATE'].dt.year\n",
        "order_data['MONTH'] = order_data['ORDER_CREATED_DATE'].dt.month\n",
        "order_data['DAY'] = order_data['ORDER_CREATED_DATE'].dt.day\n",
        "order_data.drop('ORDER_CREATED_DATE', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bfec7be",
      "metadata": {},
      "source": [
        "2. Efficient JSON Parsing and Flattening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "902bd572",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "902bd572",
        "outputId": "49432bea-c9e1-4b52-d0fd-98fe5b272d58"
      },
      "outputs": [],
      "source": [
        "# Define pattern for items to exclude\n",
        "pattern_to_exclude = r\"^(Order|Delivery Fee|Sub Box|Unavailable Item|Plastic|Extra Sauce|Ketchup Pack|Seasoning Pack)\"\n",
        "\n",
        "# Parse 'ORDERS' JSON directly into a new column\n",
        "order_data['PARSED_ORDERS'] = order_data['ORDERS'].apply(json.loads)\n",
        "\n",
        "# Extract 'item_details' list\n",
        "order_data['ITEM_DETAILS_LIST'] = order_data['PARSED_ORDERS'].apply(lambda x: x['orders'][0]['item_details'])\n",
        "\n",
        "# Explode the DataFrame to have one row per item within an order\n",
        "# Select only necessary columns before exploding to minimize memory footprint\n",
        "# and later merge with customer data.\n",
        "processed_items = order_data[[\n",
        "    'CUSTOMER_ID', 'STORE_NUMBER', 'ORDER_ID',\n",
        "    'ORDER_CHANNEL_NAME', 'ORDER_SUBCHANNEL_NAME', 'ORDER_OCCASION_NAME',\n",
        "    'YEAR', 'MONTH', 'DAY', 'ITEM_DETAILS_LIST'\n",
        "]].explode('ITEM_DETAILS_LIST')\n",
        "\n",
        "# Extract item details into separate columns using .apply(get) - efficient\n",
        "processed_items['ITEM_NAME'] = processed_items['ITEM_DETAILS_LIST'].apply(lambda x: x.get('item_name'))\n",
        "processed_items['ITEM_PRICE'] = processed_items['ITEM_DETAILS_LIST'].apply(lambda x: x.get('item_price')).astype(float)\n",
        "processed_items['ITEM_QUANTITY'] = processed_items['ITEM_DETAILS_LIST'].apply(lambda x: x.get('item_quantity')).astype(int)\n",
        "\n",
        "# Filter out excluded items (using the regex pattern)\n",
        "processed_items = processed_items[~processed_items['ITEM_NAME'].str.contains(pattern_to_exclude, na=False, regex=True)]\n",
        "\n",
        "# Drop the intermediate list column\n",
        "processed_items.drop(columns=['ITEM_DETAILS_LIST'], inplace=True)\n",
        "\n",
        "train_data_dl = pd.merge(\n",
        "    processed_items,\n",
        "    customer_data[['CUSTOMER_ID', 'CUSTOMER_TYPE']],\n",
        "    on='CUSTOMER_ID',\n",
        "    how='left'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b001a038",
      "metadata": {},
      "source": [
        "3. Calculate Cart-Level Numerical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa1e579e",
      "metadata": {
        "id": "fa1e579e"
      },
      "outputs": [],
      "source": [
        "train_data_dl['CART_AVG_ITEM_PRICE'] = train_data_dl.groupby('ORDER_ID')['ITEM_PRICE'].transform('mean')\n",
        "train_data_dl['CART_TOTAL_QUANTITY'] = train_data_dl.groupby('ORDER_ID')['ITEM_QUANTITY'].transform('sum')\n",
        "train_data_dl['CART_NUM_UNIQUE_ITEMS'] = train_data_dl.groupby('ORDER_ID')['ITEM_NAME'].transform('nunique')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a1f037",
      "metadata": {},
      "source": [
        "4. Encoding Categorical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97914ced",
      "metadata": {
        "id": "97914ced"
      },
      "outputs": [],
      "source": [
        "# Initialize all encoders\n",
        "item_name_encoder = LabelEncoder()\n",
        "order_id_encoder = LabelEncoder()\n",
        "customer_id_encoder = LabelEncoder()\n",
        "customer_type_encoder = LabelEncoder()\n",
        "order_channel_name_encoder = LabelEncoder()\n",
        "order_subchannel_name_encoder = LabelEncoder()\n",
        "order_occasion_name_encoder = LabelEncoder()\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit ALL LabelEncoders on the entire relevant columns from train_data_dl\n",
        "train_data_dl['ITEM_ID'] = item_name_encoder.fit_transform(train_data_dl['ITEM_NAME'])\n",
        "train_data_dl['ORDER_ID_ENCODED'] = order_id_encoder.fit_transform(train_data_dl['ORDER_ID'])\n",
        "train_data_dl['CUSTOMER_ID_ENCODED'] = customer_id_encoder.fit_transform(train_data_dl['CUSTOMER_ID'])\n",
        "train_data_dl['ORDER_CHANNEL_NAME_ENCODED'] = order_channel_name_encoder.fit_transform(train_data_dl['ORDER_CHANNEL_NAME'])\n",
        "train_data_dl['ORDER_SUBCHANNEL_NAME_ENCODED'] = order_subchannel_name_encoder.fit_transform(train_data_dl['ORDER_SUBCHANNEL_NAME'])\n",
        "train_data_dl['ORDER_OCCASION_NAME_ENCODED'] = order_occasion_name_encoder.fit_transform(train_data_dl['ORDER_OCCASION_NAME'])\n",
        "\n",
        "# For CUSTOMER_TYPE, fit on unique values from the original customer_data to maintain consistency and then transform.\n",
        "customer_type_encoder.fit(customer_data['CUSTOMER_TYPE'].unique())\n",
        "train_data_dl['CUSTOMER_TYPE_ENCODED'] = customer_type_encoder.transform(train_data_dl['CUSTOMER_TYPE'])\n",
        "\n",
        "# Create mappings for decoding later\n",
        "customer_id_map_dl = {idx: original_id for idx, original_id in enumerate(customer_id_encoder.classes_)}\n",
        "item_name_map_dl = {idx: original_name for idx, original_name in enumerate(item_name_encoder.classes_)}\n",
        "customer_type_map = {idx: original_type for idx, original_type in enumerate(customer_type_encoder.classes_)}\n",
        "order_channel_map = {idx: name for idx, name in enumerate(order_channel_name_encoder.classes_)}\n",
        "order_subchannel_map = {idx: name for idx, name in enumerate(order_subchannel_name_encoder.classes_)}\n",
        "order_occasion_map = {idx: name for idx, name in enumerate(order_occasion_name_encoder.classes_)}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a934ac1",
      "metadata": {},
      "source": [
        "5. Calculate MAX_CART_SIZE and CART_ITEM_IDS_ENCODED_PADDED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1636b809",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1636b809",
        "outputId": "39df03ab-0e2a-4c21-9abd-9a613d119c00"
      },
      "outputs": [],
      "source": [
        "# Determine MAX_CART_SIZE from the processed data\n",
        "MAX_CART_SIZE = train_data_dl.groupby('ORDER_ID')['ITEM_NAME'].nunique().max()\n",
        "print(f\"Determined MAX_CART_SIZE: {MAX_CART_SIZE}\")\n",
        "\n",
        "# Helper function for padding (moved outside to be simpler)\n",
        "def pad_encoded_item_ids(item_ids, max_size, padding_value=0):\n",
        "    padded_ids = np.full(max_size, padding_value, dtype=int)\n",
        "    actual_len = len(item_ids)\n",
        "    padded_ids[:min(actual_len, max_size)] = item_ids[:min(actual_len, max_size)]\n",
        "    return padded_ids\n",
        "\n",
        "# Pre-calculate padded item IDs for each unique ORDER_ID\n",
        "# We'll use the item_name_encoder directly here.\n",
        "order_item_encoded_ids = train_data_dl.groupby('ORDER_ID')['ITEM_ID'].apply(list)\n",
        "\n",
        "# Apply padding to each list of encoded item IDs\n",
        "padded_item_ids_per_order_series = order_item_encoded_ids.apply(\n",
        "    lambda x: pad_encoded_item_ids(np.array(x), MAX_CART_SIZE)\n",
        ")\n",
        "\n",
        "# Map these pre-calculated padded IDs back to the original train_data_dl DataFrame\n",
        "train_data_dl['CART_ITEM_IDS_ENCODED_PADDED'] = train_data_dl['ORDER_ID'].map(padded_item_ids_per_order_series)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56312ec0",
      "metadata": {},
      "source": [
        "6. Scale Numerical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145c50f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "145c50f6",
        "outputId": "5ea2d229-8afa-46a8-9e07-e01602db1363"
      },
      "outputs": [],
      "source": [
        "numerical_features = ['ITEM_PRICE', 'CART_AVG_ITEM_PRICE', 'CART_TOTAL_QUANTITY', 'CART_NUM_UNIQUE_ITEMS']\n",
        "train_data_dl[numerical_features] = scaler.fit_transform(train_data_dl[numerical_features])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22686f58",
      "metadata": {},
      "source": [
        "7. Define Input Features and Target, then Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23e7d23b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23e7d23b",
        "outputId": "9617dd48-dbff-4f95-9027-b236dd11da31"
      },
      "outputs": [],
      "source": [
        "categorical_cols = [\n",
        "    'CUSTOMER_ID_ENCODED', 'ITEM_ID', 'ORDER_CHANNEL_NAME_ENCODED',\n",
        "    'ORDER_SUBCHANNEL_NAME_ENCODED', 'ORDER_OCCASION_NAME_ENCODED', 'CUSTOMER_TYPE_ENCODED'\n",
        "]\n",
        "\n",
        "numerical_cols = ['ITEM_PRICE', 'CART_AVG_ITEM_PRICE', 'CART_TOTAL_QUANTITY', 'CART_NUM_UNIQUE_ITEMS']\n",
        "\n",
        "cart_item_embedding_col = ['CART_ITEM_IDS_ENCODED_PADDED']\n",
        "\n",
        "target_col = 'ITEM_QUANTITY'\n",
        "\n",
        "X = train_data_dl[categorical_cols + numerical_cols + cart_item_embedding_col]\n",
        "Y = train_data_dl[target_col]\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nTraining data shape with all features: {X_train.shape}, Target shape: {Y_train.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adb84807",
      "metadata": {
        "id": "adb84807"
      },
      "source": [
        "#### Hybrid Deep Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad30c1cb",
      "metadata": {},
      "source": [
        "1. Define Vocabulary Sizes & Embedding Dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ab6cf23",
      "metadata": {
        "id": "1ab6cf23"
      },
      "outputs": [],
      "source": [
        "# --- Define Vocabulary Sizes (Cardinalities) ---\n",
        "num_customers = len(customer_id_encoder.classes_)\n",
        "num_items = len(item_name_encoder.classes_)\n",
        "num_order_channels = len(order_channel_name_encoder.classes_)\n",
        "num_order_subchannels = len(order_subchannel_name_encoder.classes_)\n",
        "num_order_occasions = len(order_occasion_name_encoder.classes_)\n",
        "num_customer_types = len(customer_type_encoder.classes_)\n",
        "\n",
        "# --- Define Embedding Dimensions (Hyperparameters) ---\n",
        "customer_embedding_dim = min(50, num_customers // 2) if num_customers > 1 else 1\n",
        "item_embedding_dim = min(50, num_items // 2) if num_items > 1 else 1\n",
        "channel_embedding_dim = min(10, num_order_channels // 2) if num_order_channels > 1 else 1\n",
        "subchannel_embedding_dim = min(10, num_order_subchannels // 2) if num_order_subchannels > 1 else 1\n",
        "occasion_embedding_dim = min(10, num_order_occasions // 2) if num_order_occasions > 1 else 1\n",
        "customer_type_embedding_dim = min(10, num_customer_types // 2) if num_customer_types > 1 else 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a83c386",
      "metadata": {},
      "source": [
        "2. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a53526d",
      "metadata": {
        "id": "8a53526d"
      },
      "outputs": [],
      "source": [
        "# 1. Input Layers for each feature\n",
        "customer_id_input = Input(shape=(1,), name='customer_id_input')\n",
        "item_id_input = Input(shape=(1,), name='item_id_input')\n",
        "order_channel_input = Input(shape=(1,), name='order_channel_input')\n",
        "order_subchannel_input = Input(shape=(1,), name='order_subchannel_input')\n",
        "order_occasion_input = Input(shape=(1,), name='order_occasion_input')\n",
        "customer_type_input = Input(shape=(1,), name='customer_type_input')\n",
        "item_price_input = Input(shape=(1,), name='item_price_input')\n",
        "cart_items_input = Input(shape=(MAX_CART_SIZE,), name='cart_items_input')\n",
        "\n",
        "# 2. Embedding Layers for categorical features\n",
        "customer_embedding = Embedding(input_dim=num_customers, output_dim=customer_embedding_dim, name='customer_embedding')(customer_id_input)\n",
        "item_embedding = Embedding(input_dim=num_items, output_dim=item_embedding_dim, name='item_embedding')(item_id_input)\n",
        "order_channel_embedding = Embedding(input_dim=num_order_channels, output_dim=channel_embedding_dim, name='order_channel_embedding')(order_channel_input)\n",
        "order_subchannel_embedding = Embedding(input_dim=num_order_subchannels, output_dim=subchannel_embedding_dim, name='order_subchannel_embedding')(order_subchannel_input)\n",
        "order_occasion_embedding = Embedding(input_dim=num_order_occasions, output_dim=occasion_embedding_dim, name='order_occasion_embedding')(order_occasion_input)\n",
        "customer_type_embedding = Embedding(input_dim=num_customer_types, output_dim=customer_type_embedding_dim, name='customer_type_embedding')(customer_type_input)\n",
        "cart_items_embedded = Embedding(input_dim=num_items, output_dim=item_embedding_dim, name='cart_items_embedding')(cart_items_input)\n",
        "\n",
        "# Average pooling for cart items\n",
        "cart_items_pooled = GlobalAveragePooling1D(name='cart_items_pooled_embedding')(cart_items_embedded) \n",
        "\n",
        "# Flatten other embedding outputs\n",
        "customer_vec = Flatten()(customer_embedding)\n",
        "item_vec = Flatten()(item_embedding)\n",
        "order_channel_vec = Flatten()(order_channel_embedding)\n",
        "order_subchannel_vec = Flatten()(order_subchannel_embedding)\n",
        "order_occasion_vec = Flatten()(order_occasion_embedding)\n",
        "customer_type_vec = Flatten()(customer_type_embedding)\n",
        "\n",
        "# 3. Concatenate all features (flattened embeddings + numerical inputs + pooled cart items)\n",
        "concatenated_features = Concatenate()(\n",
        "    [customer_vec, item_vec, order_channel_vec, order_subchannel_vec,\n",
        "     order_occasion_vec, customer_type_vec,\n",
        "     item_price_input, cart_items_pooled] # Added cart_items_pooled here\n",
        ")\n",
        "\n",
        "# 4. Deep Neural Network (DNN) layers\n",
        "dense_1 = Dense(128, activation='relu')(concatenated_features)\n",
        "dense_2 = Dense(64, activation='relu')(dense_1)\n",
        "dense_3 = Dense(32, activation='relu')(dense_2)\n",
        "\n",
        "# 5. Output Layer: Predict ITEM_QUANTITY (regression task)\n",
        "output_layer = Dense(1, activation='relu', name='item_quantity_prediction')(dense_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798e0650",
      "metadata": {},
      "source": [
        "3. Initialization and Compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a307e0ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a307e0ad",
        "outputId": "9c4d03c5-a584-425f-84b0-10626c42df5e"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "model = Model(\n",
        "    inputs=[customer_id_input, item_id_input, order_channel_input, order_subchannel_input,\n",
        "            order_occasion_input, customer_type_input,\n",
        "            item_price_input, cart_items_input], # Updated inputs\n",
        "    outputs=output_layer\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), \n",
        "    loss=tf.keras.losses.Huber(), \n",
        "    metrics=[\n",
        "            tf.keras.metrics.MeanAbsoluteError(name=\"mae\"),\n",
        "            tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"),\n",
        "        ])\n",
        "\n",
        "print(\"Deep Learning Model Summary (with Padded Cart Items):\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21089b5e",
      "metadata": {},
      "source": [
        "4. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d44195f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d44195f6",
        "outputId": "e8b6d5b8-fa1b-4c83-a2d0-6a3762fc4952"
      },
      "outputs": [],
      "source": [
        "# Map X_train, X_val to the dictionary format expected by Keras\n",
        "def to_input_dict(df):\n",
        "    return {\n",
        "        'customer_id_input': df['CUSTOMER_ID_ENCODED'],\n",
        "        'item_id_input': df['ITEM_ID'],\n",
        "        'order_channel_input': df['ORDER_CHANNEL_NAME_ENCODED'],\n",
        "        'order_subchannel_input': df['ORDER_SUBCHANNEL_NAME_ENCODED'],\n",
        "        'order_occasion_input': df['ORDER_OCCASION_NAME_ENCODED'],\n",
        "        'customer_type_input': df['CUSTOMER_TYPE_ENCODED'],\n",
        "        'item_price_input': df['ITEM_PRICE'],\n",
        "        'cart_items_input': np.array(df['CART_ITEM_IDS_ENCODED_PADDED'].tolist()) # Convert list of lists to numpy array\n",
        "    }\n",
        "\n",
        "train_inputs = to_input_dict(X_train)\n",
        "val_inputs = to_input_dict(X_val)\n",
        "\n",
        "train_inputs = to_input_dict(X_train)\n",
        "val_inputs = to_input_dict(X_val)\n",
        "\n",
        "training = False\n",
        "if training:\n",
        "    print(\"\\nTraining Deep Learning Model...\")\n",
        "    history = model.fit(\n",
        "        train_inputs,\n",
        "        Y_train,\n",
        "        epochs=5, # Reduced epochs for quick demonstration\n",
        "        batch_size=64,\n",
        "        validation_data=(val_inputs, Y_val),\n",
        "        verbose=1\n",
        "    )\n",
        "    print(\"Model Training Complete.\")\n",
        "    model.save_weights(\"model6_v1.weights.h5\")\n",
        "else:\n",
        "    print(\"\\nLoading Deep Learning Model weights...\")\n",
        "    model.load_weights(\"model6_v1.weights.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5bffd1f",
      "metadata": {},
      "source": [
        "5. Loss & Metrics visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SQRBjb8Khg-j",
      "metadata": {
        "id": "SQRBjb8Khg-j"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plots training and validation metrics from a Keras History object.\n",
        "    \"\"\"\n",
        "    hist = history.history\n",
        "    metrics = [m for m in hist.keys() if not m.startswith('val_')]\n",
        "\n",
        "    for metric in metrics:\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(hist[metric], label=f'Training {metric}')\n",
        "        if f'val_{metric}' in hist:\n",
        "            plt.plot(hist[f'val_{metric}'], label=f'Validation {metric}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel(metric.capitalize())\n",
        "        plt.title(f'Training and Validation {metric.capitalize()}')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "# Example usage:\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "827f1798",
      "metadata": {
        "id": "827f1798"
      },
      "outputs": [],
      "source": [
        "def pad_cart_items(encoded_item_ids, max_size, padding_value=0):\n",
        "    \"\"\"Pads a list of encoded item IDs to a fixed size.\"\"\"\n",
        "    if len(encoded_item_ids) >= max_size:\n",
        "        return encoded_item_ids[:max_size]\n",
        "    else:\n",
        "        return encoded_item_ids + [padding_value] * (max_size - len(encoded_item_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZV5eO7H3WRtR",
      "metadata": {
        "id": "ZV5eO7H3WRtR"
      },
      "outputs": [],
      "source": [
        "item_features = (\n",
        "    train_data_dl\n",
        "    .groupby('ITEM_NAME', as_index=True)\n",
        "    .agg({'ITEM_PRICE': 'first', 'ITEM_QUANTITY': 'first'})\n",
        "    .to_dict(orient='index')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "601f564b",
      "metadata": {
        "id": "601f564b"
      },
      "outputs": [],
      "source": [
        "# --- Helper Function: Calculate Cart Features from item names ---\n",
        "# This function is crucial for creating the 'cart context' for new predictions.\n",
        "def calculate_cart_features_for_prediction(cart_item_names, item_encoder, scaler):\n",
        "    \"\"\"\n",
        "    Calculates aggregated cart features (avg_price, total_qty, num_unique)\n",
        "    from a list of item names and scales them using the pre-fitted scaler.\n",
        "\n",
        "    Args:\n",
        "        cart_item_names (list): List of item names currently in the cart.\n",
        "        item_encoder (LabelEncoder): Fitted encoder for item names.\n",
        "        scaler (MinMaxScaler): Fitted scaler for numerical features.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing scaled cart features.\n",
        "    \"\"\"\n",
        "    temp_item_data = []\n",
        "    # Fetch details for each item in the cart from your full item catalog/features\n",
        "    # In a real system, you'd look these up from an item database.\n",
        "    # For this example, we'll use a mock item features dictionary.\n",
        "\n",
        "    for item_name in cart_item_names:\n",
        "        if item_name in item_features: # Check if item is known\n",
        "            temp_item_data.append(item_features[item_name])\n",
        "        else:\n",
        "            print(f\"Warning: Cart item '{item_name}' not found in item features. Skipping.\")\n",
        "\n",
        "    if not temp_item_data:\n",
        "        # If cart is empty or all items are unknown, use default/average values for features\n",
        "        # These averages should ideally come from your training data's unscaled means.\n",
        "        # Here using arbitrary defaults for demonstration.\n",
        "        avg_price_default = 5.0\n",
        "        total_qty_default = 1.0\n",
        "        num_unique_default = 0\n",
        "\n",
        "        cart_feature_values = {\n",
        "            'ITEM_PRICE': avg_price_default, # This will be the reference point for the target item's price\n",
        "            'CART_AVG_ITEM_PRICE': avg_price_default,\n",
        "            'CART_TOTAL_QUANTITY': total_qty_default,\n",
        "            'CART_NUM_UNIQUE_ITEMS': num_unique_default\n",
        "        }\n",
        "    else:\n",
        "        cart_df_temp = pd.DataFrame(temp_item_data)\n",
        "        cart_feature_values = {\n",
        "            'ITEM_PRICE': cart_df_temp['ITEM_PRICE'].mean(), # Placeholder, this will be replaced by candidate item's price\n",
        "            'CART_AVG_ITEM_PRICE': cart_df_temp['ITEM_PRICE'].mean(),\n",
        "            'CART_TOTAL_QUANTITY': cart_df_temp['ITEM_QUANTITY'].sum(),\n",
        "            'CART_NUM_UNIQUE_ITEMS': cart_df_temp['ITEM_NAME'].nunique() if 'ITEM_NAME' in cart_df_temp.columns else len(cart_item_names) # Use actual name if available\n",
        "        }\n",
        "\n",
        "    # Create a dummy DataFrame matching the numerical_features column order for scaling\n",
        "    numerical_features_order = ['ITEM_PRICE', 'CART_AVG_ITEM_PRICE', 'CART_TOTAL_QUANTITY', 'CART_NUM_UNIQUE_ITEMS']\n",
        "    dummy_input_for_scaler = pd.DataFrame([cart_feature_values], columns=numerical_features_order)\n",
        "\n",
        "    scaled_values = scaler.transform(dummy_input_for_scaler).flatten()\n",
        "\n",
        "    # Return a dictionary of scaled cart features, extracted by name\n",
        "    return {\n",
        "        'CART_AVG_ITEM_PRICE': scaled_values[numerical_features_order.index('CART_AVG_ITEM_PRICE')],\n",
        "        'CART_TOTAL_QUANTITY': scaled_values[numerical_features_order.index('CART_TOTAL_QUANTITY')],\n",
        "        'CART_NUM_UNIQUE_ITEMS': scaled_values[numerical_features_order.index('CART_NUM_UNIQUE_ITEMS')]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deba0bec",
      "metadata": {
        "id": "deba0bec"
      },
      "outputs": [],
      "source": [
        "def dl_recommendations_with_cart(original_customer_id, cart_item_names, num_recommendations=5):\n",
        "    \"\"\"\n",
        "    Generates recommendations for a customer based on their ID and current cart items\n",
        "    using the trained deep learning model with padded cart item inputs.\n",
        "\n",
        "    Args:\n",
        "        original_customer_id (int): The customer's original ID.\n",
        "        cart_item_names (list): List of item names currently in the user's cart.\n",
        "        num_recommendations (int): Number of top recommendations to return.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: Recommended item names.\n",
        "    \"\"\"\n",
        "    # 1. Encode Customer ID\n",
        "    try:\n",
        "        customer_id_encoded = customer_id_encoder.transform([original_customer_id])[0]\n",
        "    except ValueError:\n",
        "        print(f\"Customer ID {original_customer_id} not known to the model. Using a default/average customer ID.\")\n",
        "        customer_id_encoded = customer_id_encoder.classes_[0]\n",
        "\n",
        "    # 2. Identify Candidate Items (not purchased, not in cart)\n",
        "    purchased_items_encoded = []\n",
        "    if original_customer_id in train_data_dl['CUSTOMER_ID'].unique():\n",
        "        purchased_items_encoded = train_data_dl[train_data_dl['CUSTOMER_ID'] == original_customer_id]['ITEM_ID'].tolist()\n",
        "\n",
        "    cart_item_ids_encoded_unpadded = []\n",
        "    for item_name in cart_item_names:\n",
        "        try:\n",
        "            cart_item_ids_encoded_unpadded.append(item_name_encoder.transform([item_name])[0])\n",
        "        except ValueError:\n",
        "            pass # Item not in our vocabulary, ignore\n",
        "\n",
        "    excluded_items_encoded = set(purchased_items_encoded + cart_item_ids_encoded_unpadded)\n",
        "\n",
        "    all_known_item_ids_encoded = item_name_encoder.transform(list(item_name_map_dl.values())) # Convert to list\n",
        "    unseen_item_ids_encoded = [item_id for item_id in all_known_item_ids_encoded if item_id not in excluded_items_encoded]\n",
        "\n",
        "    if not unseen_item_ids_encoded:\n",
        "        print(f\"User {original_customer_id} has purchased/carted all known items or no unseen items to recommend.\")\n",
        "        return pd.Series([])\n",
        "\n",
        "    num_unseen = len(unseen_item_ids_encoded)\n",
        "\n",
        "    # 3. Prepare Context Features for Prediction\n",
        "    # Pad the cart items for the model input\n",
        "    padded_cart_items_encoded = pad_cart_items(cart_item_ids_encoded_unpadded, MAX_CART_SIZE)\n",
        "\n",
        "    # Replicate user-specific and other general context features for each unseen item\n",
        "    user_context_row = train_data_dl[train_data_dl['CUSTOMER_ID_ENCODED'] == customer_id_encoded].iloc[0] if customer_id_encoded in train_data_dl['CUSTOMER_ID_ENCODED'].unique() else None\n",
        "\n",
        "    customer_type_for_pred = user_context_row['CUSTOMER_TYPE_ENCODED'] if user_context_row is not None else customer_type_encoder.transform([customer_data['CUSTOMER_TYPE'].mode()[0]])[0]\n",
        "    channel_for_pred = user_context_row['ORDER_CHANNEL_NAME_ENCODED'] if user_context_row is not None else order_channel_name_encoder.transform([train_data_dl['ORDER_CHANNEL_NAME'].mode()[0]])[0]\n",
        "    subchannel_for_pred = user_context_row['ORDER_SUBCHANNEL_NAME_ENCODED'] if user_context_row is not None else order_subchannel_name_encoder.transform([train_data_dl['ORDER_SUBCHANNEL_NAME'].mode()[0]])[0]\n",
        "    occasion_for_pred = user_context_row['ORDER_OCCASION_NAME_ENCODED'] if user_context_row is not None else order_occasion_name_encoder.transform([train_data_dl['ORDER_OCCASION_NAME'].mode()[0]])[0] # Corrected column name\n",
        "\n",
        "    # Replicate all inputs for prediction\n",
        "    pred_customer_ids = np.full(num_unseen, customer_id_encoded)\n",
        "    pred_item_ids = np.array(unseen_item_ids_encoded)\n",
        "\n",
        "    # Fetch and scale item prices for each candidate item\n",
        "    candidate_item_prices = []\n",
        "    for item_id_enc in unseen_item_ids_encoded:\n",
        "        item_name = item_name_map_dl.get(item_id_enc, f\"Unknown_Item_{item_id_enc}\")\n",
        "        price = item_features[item_name]['ITEM_PRICE']\n",
        "        candidate_item_prices.append(price)\n",
        "\n",
        "    # Create a dummy DataFrame with only 'ITEM_PRICE' for the scaler, using the same column names as `numerical_features`\n",
        "    numerical_features_order = ['ITEM_PRICE', 'CART_AVG_ITEM_PRICE', 'CART_TOTAL_QUANTITY', 'CART_NUM_UNIQUE_ITEMS'] # Need all numerical columns for the scaler\n",
        "    dummy_input_for_scaler = pd.DataFrame(np.zeros((len(candidate_item_prices), len(numerical_features_order))), columns=numerical_features_order)\n",
        "    dummy_input_for_scaler['ITEM_PRICE'] = candidate_item_prices\n",
        "\n",
        "    # Also need to calculate and add the cart features for each prediction row\n",
        "    # These will be the same for all candidate items for a given customer and cart\n",
        "    cart_features_scaled = calculate_cart_features_for_prediction(cart_item_names, item_name_encoder, scaler)\n",
        "\n",
        "    dummy_input_for_scaler['CART_AVG_ITEM_PRICE'] = cart_features_scaled['CART_AVG_ITEM_PRICE']\n",
        "    dummy_input_for_scaler['CART_TOTAL_QUANTITY'] = cart_features_scaled['CART_TOTAL_QUANTITY']\n",
        "    dummy_input_for_scaler['CART_NUM_UNIQUE_ITEMS'] = cart_features_scaled['CART_NUM_UNIQUE_ITEMS']\n",
        "\n",
        "\n",
        "    # Scale all numerical features\n",
        "    scaled_numerical_values = scaler.transform(dummy_input_for_scaler)\n",
        "\n",
        "    pred_item_prices_scaled = scaled_numerical_values[:, numerical_features_order.index('ITEM_PRICE')]\n",
        "    pred_cart_avg_item_price_scaled = scaled_numerical_values[:, numerical_features_order.index('CART_AVG_ITEM_PRICE')]\n",
        "    pred_cart_total_quantity_scaled = scaled_numerical_values[:, numerical_features_order.index('CART_TOTAL_QUANTITY')]\n",
        "    pred_cart_num_unique_items_scaled = scaled_numerical_values[:, numerical_features_order.index('CART_NUM_UNIQUE_ITEMS')]\n",
        "\n",
        "\n",
        "    # Replicate general context and cart items\n",
        "    pred_channels = np.full(num_unseen, channel_for_pred)\n",
        "    pred_subchannels = np.full(num_unseen, subchannel_for_pred)\n",
        "    pred_occasions = np.full(num_unseen, occasion_for_pred)\n",
        "    pred_customer_types = np.full(num_unseen, customer_type_for_pred)\n",
        "\n",
        "    # Replicate padded cart items for all unseen items\n",
        "    pred_padded_cart_items = np.tile(np.array(padded_cart_items_encoded), (num_unseen, 1))\n",
        "\n",
        "    # 4. Create prediction input dictionary for the Keras model\n",
        "    prediction_inputs = {\n",
        "        'customer_id_input': pred_customer_ids,\n",
        "        'item_id_input': pred_item_ids,\n",
        "        'order_channel_input': pred_channels,\n",
        "        'order_subchannel_input': pred_subchannels,\n",
        "        'order_occasion_input': pred_occasions,\n",
        "        'customer_type_input': pred_customer_types,\n",
        "        'item_price_input': pred_item_prices_scaled,\n",
        "        'cart_items_input': pred_padded_cart_items\n",
        "    }\n",
        "\n",
        "    # 5. Predict scores\n",
        "    predicted_quantities = model.predict(prediction_inputs).flatten()\n",
        "\n",
        "    # Create a Series of scores with item_id_encoded as index\n",
        "    item_scores = pd.Series(predicted_quantities, index=unseen_item_ids_encoded)\n",
        "\n",
        "    # 6. Sort and get top N recommendations\n",
        "    top_n_encoded_item_ids = item_scores.sort_values(ascending=False).head(num_recommendations).index.tolist()\n",
        "\n",
        "    # Map back to original item names\n",
        "    recommended_item_names = [item_name_map_dl.get(idx, f\"Unknown_Item_{idx}\") for idx in top_n_encoded_item_ids]\n",
        "\n",
        "    return pd.Series(recommended_item_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3109c48",
      "metadata": {},
      "source": [
        "#### Test Questions Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e51edbe8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "e51edbe8",
        "outputId": "2d43b06a-57a0-4018-a7e9-69ee50f7cf9d"
      },
      "outputs": [],
      "source": [
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c52f62",
      "metadata": {
        "id": "18c52f62"
      },
      "source": [
        "1. Prepare Test Data for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31a51a94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31a51a94",
        "outputId": "c81fe301-4cf8-40ee-c655-0996da1eb866"
      },
      "outputs": [],
      "source": [
        "test_cases = []\n",
        "for index, row in test_data.iterrows():\n",
        "    customer_id = row['CUSTOMER_ID']\n",
        "    # Items in the test order (item1, item2, item3) will serve as both\n",
        "    # the 'cart_items' (context for prediction) and 'actual_purchased_items' (ground truth)\n",
        "    cart_and_purchased_items = [\n",
        "        row['item1'], row['item2'], row['item3']\n",
        "    ]\n",
        "\n",
        "    # Filter out any NaN values that might exist if an order had less than 3 items\n",
        "    cart_and_purchased_items = [item for item in cart_and_purchased_items if pd.notna(item)]\n",
        "\n",
        "    test_cases.append({\n",
        "        'customer_id': customer_id,\n",
        "        'cart_items': cart_and_purchased_items,\n",
        "    })\n",
        "\n",
        "print(f\"Prepared {len(test_cases)} test cases.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5559fc7",
      "metadata": {
        "id": "a5559fc7"
      },
      "source": [
        "2. Generate Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f38d92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85f38d92",
        "outputId": "de071a56-70a0-486c-ea3e-646db37aa132"
      },
      "outputs": [],
      "source": [
        "# Generate recommendations for each test case\n",
        "recommendations = []\n",
        "\n",
        "for test_case in test_cases:\n",
        "    customer_id = test_case['customer_id']\n",
        "    cart_items = test_case['cart_items']\n",
        "\n",
        "    # Generate top 3 recommendations\n",
        "    recs = dl_recommendations_with_cart(customer_id, cart_items, num_recommendations=3)\n",
        "\n",
        "    recommendations.append({\n",
        "        'customer_id': customer_id,\n",
        "        'cart_items': cart_items,\n",
        "        'recommended_items': recs.tolist() # Convert Series to list for easier processing\n",
        "    })\n",
        "\n",
        "# Convert recommendations list of dicts to DataFrame\n",
        "recs_df = pd.DataFrame(recommendations)\n",
        "\n",
        "# Expand the recommended_items list into separate columns\n",
        "recs_expanded = pd.DataFrame(recs_df['recommended_items'].tolist(),\n",
        "                             columns=['RECOMMENDATION 1', 'RECOMMENDATION 2', 'RECOMMENDATION 3'])\n",
        "\n",
        "# Combine with submission_data\n",
        "submission_data = pd.concat([test_data.reset_index(drop=True), recs_expanded], axis=1)\n",
        "\n",
        "print(f\"Generated recommendations for {len(recommendations)} test cases.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_1iIReAatyDQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "_1iIReAatyDQ",
        "outputId": "705a7250-886f-42ee-f2c5-d8e7d9cf0863"
      },
      "outputs": [],
      "source": [
        "submission_data['RECOMMENDATION 1'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5H3qbHkt7lL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "f5H3qbHkt7lL",
        "outputId": "05f533fa-0408-460a-dbd3-68460ce99b19"
      },
      "outputs": [],
      "source": [
        "submission_data['RECOMMENDATION 2'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P5hpoGUbgAYt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "P5hpoGUbgAYt",
        "outputId": "7857cdf0-8945-4291-c20b-113bdb5ecf16"
      },
      "outputs": [],
      "source": [
        "submission_data['RECOMMENDATION 3'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24643eda",
      "metadata": {},
      "outputs": [],
      "source": [
        "output_data = submission_data[['CUSTOMER_ID', 'ORDER_ID', 'item1', 'item2', 'item3',\n",
        "             'RECOMMENDATION 1', 'RECOMMENDATION 2', 'RECOMMENDATION 3']].copy()\n",
        "\n",
        "output_data.insert(output_data.columns.get_loc('item3') + 1, 'item4', 'Missing')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd9e336",
      "metadata": {},
      "outputs": [],
      "source": [
        "output_data.to_csv(\"output_data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ea5a01",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
